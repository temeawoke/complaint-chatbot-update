# CFPB Complaint Chatbot with Semantic Search

This project builds a complaint analysis and retrieval system using the **Consumer Financial Protection Bureau (CFPB)** complaint dataset. It includes data preprocessing, text chunking, vector embedding, and similarity search using **FAISS** to power a chatbot capable of retrieving semantically relevant complaints.

---

## ğŸ” Objectives

- Perform Exploratory Data Analysis (EDA)
- Preprocess and clean complaint narratives
- Chunk long complaint texts for better semantic embedding
- Generate vector embeddings using Sentence Transformers
- Store embeddings and metadata in a FAISS vector store
- Retrieve and trace complaint information based on user queries

---

## ğŸ“ Dataset

We use the publicly available **CFPB consumer complaint dataset**, filtered to include complaints related to:

- Credit card
- Personal loan
- Buy Now Pay Later (BNPL)
- Savings account
- Money transfers

---
## ğŸ“ Project Structure
complaint-chatbot/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Original CFPB dataset
â”‚ â”œâ”€â”€ processed/ # Cleaned & filtered data
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ 1.0-eda.ipynb # Exploratory Data Analysis
â”‚ â””â”€â”€ text_chunking_embedding.ipynb # # For text chunking, # For generating embeddings, # For saving to vector store
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ eda.py # EDA script (optional)
â”‚ â”œâ”€â”€ preprocessing.py # Text cleaning & normalization
â”œâ”€â”€ .github/
â”‚ â””â”€â”€ workflows/
â”‚ â””â”€â”€ ci.yml # CI setup with GitHub Actions
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md


## ğŸ“Š Task 1: EDA & Preprocessing

- Loaded the full CFPB dataset (CSV)
- Analyzed complaint distribution by product
- Explored the distribution of narrative lengths
- Filtered to include only the five relevant product categories
- Removed records with missing complaint narratives
- Cleaned complaint text (lowercase, removed boilerplate and special characters)

---

## ğŸ§© Task 2: Chunking, Embedding & Indexing

### âœ… Chunking Strategy
Used a recursive text splitter to chunk long narratives into ~300-character blocks with 50-character overlaps for semantic consistency.

### âœ… Embedding
Generated embeddings using:
- Chosen for its strong performance-speed trade-off in semantic search tasks.

### âœ… Indexing with FAISS
- Embedded vectors stored in FAISS for fast similarity search
- Metadata stored in parallel (complaint ID, product, original chunk text)

---

## ğŸ“¦ Files Generated

| File | Description |
|------|-------------|
| `faiss_complaints.index` | FAISS vector index of complaint chunks |
| `faiss_complaints_metadata.csv` | Metadata for each vector (product, chunk, ID) |
| `notebooks/eda_and_embedding.ipynb` | Full pipeline: EDA â†’ Cleaning â†’ Chunking â†’ Embedding |
| `requirements.txt` | Python dependencies |

---

## ğŸ” Semantic Search (Example)

```python
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pandas as pd

# Load
model = SentenceTransformer('all-MiniLM-L6-v2')
faiss_index = faiss.read_index('faiss_complaints.index')
metadata_df = pd.read_csv('faiss_complaints_metadata.csv')

# Query
query = "I had issues transferring money"
query_vector = model.encode([query])

# Search
D, I = faiss_index.search(np.array(query_vector), k=5)
results = metadata_df.iloc[I[0]]
print(results[['product', 'chunk']])

git clone https://github.com/temeawoke/complaint-chatbot-.git
cd complaint-chatbot
pip install -r requirements.txt

ğŸ“š Dependencies

    pandas

    numpy

    faiss-cpu

    sentence-transformers

    tqdm

    ipywidgets (optional for Jupyter progress bars)
    
    ğŸ§  Future Improvements

    Deploy the chatbot with a front-end (e.g., Gradio or Streamlit)

    Integrate with LangChain for RAG-based answering

    Add summarization or classification layers